{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The first half of this notebook is using SKlearn MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps I have followed-\n",
    "\n",
    "1. I have read all the documents in all 20 folders. I create a string of the text in a document and pass it to \n",
    "    method createArrayOfStrings(data). \n",
    "    \n",
    "2. Here I first remove meta data which I feel is every word before the word writes in all the documents.\n",
    "\n",
    "3. Next, I have removed stopwords.\n",
    "\n",
    "4. data[] is the array consisting of all the words of all the documents and features is list of name of the classes \n",
    "    of these documents.\n",
    "\n",
    "5. The, I have first made a vocabulary of all the words and have kept only the owrds with frequency more than 80.\n",
    "\n",
    "6. Finally I have made a matrix X which has only the words which are present in the vocabulary and their\n",
    "    corresponding counts for all documents.\n",
    "\n",
    "7. X and features are split to form X_train, X_test, Y_train, y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "import os\n",
    "from sklearn import datasets as ds\n",
    "from sklearn import model_selection as ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopword={}\n",
    "s_words=open('stopwords.txt','r')\n",
    "s_words_data=s_words.read()\n",
    "for x in s_words_data.split('\\n'):\n",
    "    stopword[x]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing the meta data\n",
    "def removeIrreleventData(data):\n",
    "    pos = data.find(\"writes\")\n",
    "    data = data[pos+6:]\n",
    "#     print(data[0:10])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createArrayOfStrings(data):\n",
    "    data = removeIrreleventData(data)\n",
    "    py=data.split()\n",
    "    arr=[]\n",
    "    for x in range(len(py)):\n",
    "        word=''\n",
    "        word=py[x].replace('\\n',' ')\n",
    "        word=word.replace('\\r',' ')\n",
    "        word=word.replace('\\t',' ')\n",
    "        word=word.replace(',',' ')\n",
    "        word=word.replace('(',' ')\n",
    "        word=word.replace(').',' ')\n",
    "        word=word.replace(')',' ')\n",
    "        word=word.replace('.',' ')\n",
    "        word=word.replace(':',' ') \n",
    "        word=word.replace('!',' ') \n",
    "        word=word.replace('-',' ') \n",
    "#         word=word.replace('\"','')    \n",
    "        word=word.lower()\n",
    "        if word=='':\n",
    "            continue\n",
    "        elif word == ' ':\n",
    "            continue\n",
    "        elif word in stopword.keys():\n",
    "        \n",
    "            continue\n",
    "        else:\n",
    "            arr.append(word)\n",
    "    return arr\n",
    "\n",
    "def filterLabels(labels):\n",
    "    for x in range(len(labels)):\n",
    "        labels[x]=labels[x].replace('20_newsgroups/','')\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20_newsgroups/talk.politics.mideast\n",
      "20_newsgroups/rec.autos\n",
      "20_newsgroups/comp.sys.mac.hardware\n",
      "20_newsgroups/alt.atheism\n",
      "20_newsgroups/rec.sport.baseball\n",
      "20_newsgroups/comp.os.ms-windows.misc\n",
      "20_newsgroups/rec.sport.hockey\n",
      "20_newsgroups/sci.crypt\n",
      "20_newsgroups/sci.med\n",
      "20_newsgroups/talk.politics.misc\n",
      "20_newsgroups/rec.motorcycles\n",
      "20_newsgroups/comp.windows.x\n",
      "20_newsgroups/comp.graphics\n",
      "20_newsgroups/comp.sys.ibm.pc.hardware\n",
      "20_newsgroups/sci.electronics\n",
      "20_newsgroups/talk.politics.guns\n",
      "20_newsgroups/sci.space\n",
      "20_newsgroups/soc.religion.christian\n",
      "20_newsgroups/misc.forsale\n",
      "20_newsgroups/talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = '20_newsgroups/'\n",
    "data=[]\n",
    "labels=[]\n",
    "for itemName in os.listdir(path):\n",
    "    if itemName != '.DS_Store':\n",
    "        itemName = os.path.join(path, itemName)\n",
    "        print (itemName)\n",
    "        for myPath in os.listdir(itemName):\n",
    "            myPath=os.path.join(itemName,myPath)\n",
    "            file_readed = open(myPath,encoding='latin-1')\n",
    "#         filename=open(argfile, )\n",
    "            arr=createArrayOfStrings(file_readed.read())\n",
    "            data.append(arr)\n",
    "            labels.append(itemName)\n",
    "            file_readed.close()\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lebanese', 'resistance', 'forces', 'detonated', 'bomb', 'israeli', 'occupation', 'patrol', 'lebanese', 'territory', 'two', 'days', 'ago ', 'three', 'soldiers', 'killed', 'two', 'wounded ', '\"retaliation\" ', 'israeli', 'israeli backed', 'forces', 'wounded', '8', 'civilians', 'bombarding', 'several', 'lebanese', 'villages ', 'ironically ', 'israeli', 'government', 'justifies', 'occupation', 'lebanon', 'claiming', 'necessary', 'prevent', 'bombardments', 'israeli', 'villages  ', 'congratulations', 'brave', 'men', 'lebanese', 'resistance ', 'every', 'israeli', 'son', 'place', 'grave', 'underlining', 'moral', 'bankruptcy', \"israel's\", 'occupation', 'drawing', 'attention', 'israeli', \"government's\", 'policy', 'reckless', 'disregard', 'civilian', 'life ', 'brad', 'hernlem', ' hernlem@chess ncsu edu ', 'nice ', 'three', 'people', 'murdered ', 'bradly', 'overjoyed ', 'hear', 'deaths', 'middle', 'east ', 'jewish', 'arab', 'deaths ', 'feel', 'sadness ', 'hope', 'soon', 'stops ', 'apparently ', 'view', 'point', 'acceptable', 'people', 'like', 'bradly ', 'hernlem ', 'disgust', 'me ', 'harry ']\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets as ds\n",
    "from sklearn import model_selection as ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19997\n",
      "97\n",
      "19997\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(len(data[0]))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Below I am making Training and Testing matrices.\n",
    "First, I am making vocabulary from X_train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5580\n"
     ]
    }
   ],
   "source": [
    "# Now I will make vocabulary dictionary \n",
    "vocabulary={}\n",
    "\n",
    "\n",
    "for x in data:\n",
    "    for y in x:\n",
    "        vocabulary[y]=vocabulary.get(y,0)+1 #if y not there then place 1 and y in the dictionary as Value and Key resp...\n",
    "# print(data[0][1])\n",
    "# print(type(data[0][0]))\n",
    "# these are the features for the dataset and we are going to train the GNB on this dataset now!\n",
    "\n",
    "\n",
    "# # remove the elements which have the value 1 in the dictionary\n",
    "# for key in dictionary.keys():\n",
    "#     if dictionary[key]==1:\n",
    "#         del (dictionary[key])\n",
    "vocabulary = {k: v for k, v in vocabulary.items() if v > 80}\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [1 1 1 ..., 1 1 1]\n",
      " [2 2 2 ..., 2 2 2]\n",
      " [3 3 3 ..., 3 3 3]\n",
      " [4 4 4 ..., 4 4 4]]\n",
      "20_newsgroups/talk.politics.mideast\n",
      "19997\n",
      "5580\n"
     ]
    }
   ],
   "source": [
    "# create 2d matrix of words from whole data\n",
    "n = len(vocabulary)\n",
    "m = len(data)\n",
    "# print(m)\n",
    "# print(n)\n",
    "X,noUse = np.mgrid[0:m, 0:n]\n",
    "\n",
    "print(X[0:5][0:5])\n",
    "print(labels[4])\n",
    "# print(len(X))\n",
    "# print(len(X[0]))\n",
    "for i in range(m):\n",
    "    d={}\n",
    "    for word in data[i]:\n",
    "        d[word]=d.get(word,0)+1\n",
    "    \n",
    "    j=0\n",
    "    for word in vocabulary:\n",
    "        X[i][j] = d.get(word,0)\n",
    "        if j <= n:    \n",
    "            j = j + 1\n",
    "    \n",
    "\n",
    "print(len(X))\n",
    "print(len(X[0]))        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 1 2 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [2 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "20_newsgroups/talk.politics.mideast\n"
     ]
    }
   ],
   "source": [
    "print(X[0:5][0:5])\n",
    "print(labels[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am splitting data for testing and training before making dictionary\n",
    "X_train,X_test,Y_train,Y_test=ms.train_test_split(X,labels,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf= MultinomialNB()\n",
    "# clf= GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17997\n",
      "17997\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_train))\n",
    "print(len(X_train))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(ypred.shape)\n",
    "print(len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        precision    recall  f1-score   support\n",
      "\n",
      "             20_newsgroups/alt.atheism       0.59      0.73      0.65       121\n",
      "           20_newsgroups/comp.graphics       0.79      0.79      0.79       107\n",
      " 20_newsgroups/comp.os.ms-windows.misc       0.81      0.81      0.81        95\n",
      "20_newsgroups/comp.sys.ibm.pc.hardware       0.81      0.81      0.81       103\n",
      "   20_newsgroups/comp.sys.mac.hardware       0.80      0.87      0.83       111\n",
      "          20_newsgroups/comp.windows.x       0.91      0.79      0.84        89\n",
      "            20_newsgroups/misc.forsale       0.48      0.92      0.63        90\n",
      "               20_newsgroups/rec.autos       0.82      0.82      0.82        92\n",
      "         20_newsgroups/rec.motorcycles       0.84      0.86      0.85       111\n",
      "      20_newsgroups/rec.sport.baseball       0.92      0.89      0.90       114\n",
      "        20_newsgroups/rec.sport.hockey       1.00      0.81      0.90       100\n",
      "               20_newsgroups/sci.crypt       0.87      0.88      0.88        77\n",
      "         20_newsgroups/sci.electronics       0.80      0.68      0.73        87\n",
      "                 20_newsgroups/sci.med       0.92      0.82      0.86       109\n",
      "               20_newsgroups/sci.space       0.89      0.83      0.86       112\n",
      "  20_newsgroups/soc.religion.christian       0.80      0.92      0.86        77\n",
      "      20_newsgroups/talk.politics.guns       0.71      0.81      0.76        91\n",
      "   20_newsgroups/talk.politics.mideast       0.86      0.78      0.82        97\n",
      "      20_newsgroups/talk.politics.misc       0.67      0.55      0.60       110\n",
      "      20_newsgroups/talk.religion.misc       0.59      0.31      0.40       107\n",
      "\n",
      "                           avg / total       0.79      0.78      0.78      2000\n",
      "\n",
      "[[ 88   0   0   0   1   0   4   0   4   0   0   0   0   1   1   7   0   1\n",
      "    3  11]\n",
      " [  1  85   5   0   5   2   3   0   1   0   0   1   2   0   1   0   0   0\n",
      "    1   0]\n",
      " [  0   5  77   4   3   2   2   0   0   0   0   1   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   4  83   7   0   6   0   1   0   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   1   2  97   0   6   1   0   0   0   1   3   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   9   5   1   1  70   2   0   0   0   0   0   0   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   3   0   0  83   2   0   0   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   2   1   0   8  75   2   0   0   0   1   1   1   0   0   1\n",
      "    0   0]\n",
      " [  2   0   1   0   0   1   6   4  96   0   0   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  3   0   0   0   1   1   4   0   2 101   0   0   0   0   1   0   0   1\n",
      "    0   0]\n",
      " [  0   0   0   1   0   0   5   1   2   9  81   0   0   0   0   0   0   1\n",
      "    0   0]\n",
      " [  0   0   0   0   1   1   1   0   0   0   0  68   1   0   0   0   4   0\n",
      "    1   0]\n",
      " [  1   0   1   3   2   0  12   2   1   0   0   3  59   0   2   0   0   1\n",
      "    0   0]\n",
      " [  5   2   0   0   0   0   5   1   2   0   0   0   0  89   2   0   1   1\n",
      "    0   1]\n",
      " [  1   6   0   0   0   0   2   1   1   0   0   1   0   1  93   1   0   0\n",
      "    5   0]\n",
      " [  3   0   0   0   0   0   1   0   0   0   0   0   0   0   0  71   0   0\n",
      "    0   2]\n",
      " [  1   0   0   0   0   0   7   0   0   0   0   2   0   0   0   0  74   0\n",
      "    6   1]\n",
      " [  6   0   1   1   1   0   5   0   0   0   0   0   0   0   0   1   2  76\n",
      "    4   0]\n",
      " [  2   0   0   1   1   0   4   3   2   0   0   1   0   4   2   1  16   5\n",
      "   60   8]\n",
      " [ 35   0   0   1   1   0   7   1   0   0   0   0   2   1   0   8   7   1\n",
      "   10  33]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(Y_test,ypred))\n",
    "print(confusion_matrix(Y_test,ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# My implemantation of MultinomialNB is below-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I am taking data from above in form of matrix. X_train consists of frequencies of words of training data.\n",
    "Lets say its a M*N matrix, M is number of documents in training data and N is number of words in our vocabulary.\n",
    "\n",
    "Note- I have made my vocabulary using complete data (As suggested by a TA), which I believe is a bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am making a count dictionary which has first level of keys as \"Classes\" + \"total_words\"\n",
    "# Second level of keys are all the words + \"class_total\" which is number of words in that class.\n",
    "def fit(X_train, Y_train):\n",
    "    dict1 = {}\n",
    "    classes = set(Y_train)\n",
    "    sum_all_words = 0\n",
    "    for i in range(len(X_train)):\n",
    "        current_class = Y_train[i]\n",
    "    \n",
    "        if current_class not in dict1:\n",
    "            dict1[current_class] = {}\n",
    "        class_sum = 0\n",
    "        for j in range(len(X_train[0])):\n",
    "                dict1[current_class][j] = dict1[current_class].get(j, 0) + X_train[i][j]\n",
    "                \n",
    "                sum_all_words = sum_all_words + X_train[i][j]\n",
    "                dict1[current_class][\"class_total\"] = dict1[current_class].get(\"class_total\", 0) + X_train[i][j]\n",
    "    print(sum_all_words)\n",
    "    dict1[\"total_words\"] = sum_all_words\n",
    "    return dict1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210971\n"
     ]
    }
   ],
   "source": [
    "dict1 = fit(X_test,Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.2594759623\n"
     ]
    }
   ],
   "source": [
    "print(np.log(dict1[\"total_words\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8250"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict1['20_newsgroups/comp.graphics'][\"class_total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(dict1['20_newsgroups/alt.atheism'][24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(dictionary, current_class, d):\n",
    "#     print(current_class)\n",
    "    output = np.log(dictionary[current_class][\"class_total\"]) - np.log(dictionary[\"total_words\"])\n",
    "    num_features = len(dictionary[current_class].keys()) ;\n",
    "    count_current_class = dictionary[current_class][\"class_total\"] + num_features\n",
    "    for j in range(num_features-1):\n",
    "        count_current_class_with_value_xj = dictionary[current_class][j] + 1\n",
    "        current_xj_probablity = (np.log(count_current_class_with_value_xj) - np.log(count_current_class)) * (d[j])\n",
    "        output = output + current_xj_probablity\n",
    "#     print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictOne(d, dictionary):\n",
    "    classes = dictionary.keys()\n",
    "    best_p = -1000000\n",
    "    best_class = -1\n",
    "    first_run = True\n",
    "    for current_class in classes:\n",
    "        if(current_class == \"total_words\"):\n",
    "            continue\n",
    "        p_current_class = probability(dictionary, current_class, d)\n",
    "        if (first_run or p_current_class > best_p):\n",
    "            best_p = p_current_class\n",
    "            best_class = current_class\n",
    "        first_run = False\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X_test, count):\n",
    "    Y_pred = []\n",
    "    for d in X_test:\n",
    "        Y_pred.append(predictOne(d, count))\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = predict(X_test,dict1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        precision    recall  f1-score   support\n",
      "\n",
      "             20_newsgroups/alt.atheism       0.87      0.87      0.87       121\n",
      "           20_newsgroups/comp.graphics       0.81      0.95      0.88       107\n",
      " 20_newsgroups/comp.os.ms-windows.misc       0.97      0.99      0.98        95\n",
      "20_newsgroups/comp.sys.ibm.pc.hardware       0.95      0.98      0.97       103\n",
      "   20_newsgroups/comp.sys.mac.hardware       0.80      1.00      0.89       111\n",
      "          20_newsgroups/comp.windows.x       0.98      0.96      0.97        89\n",
      "            20_newsgroups/misc.forsale       0.81      0.99      0.89        90\n",
      "               20_newsgroups/rec.autos       0.98      0.91      0.94        92\n",
      "         20_newsgroups/rec.motorcycles       0.98      0.95      0.96       111\n",
      "      20_newsgroups/rec.sport.baseball       0.99      0.92      0.95       114\n",
      "        20_newsgroups/rec.sport.hockey       0.99      0.98      0.98       100\n",
      "               20_newsgroups/sci.crypt       1.00      0.96      0.98        77\n",
      "         20_newsgroups/sci.electronics       0.99      0.97      0.98        87\n",
      "                 20_newsgroups/sci.med       0.99      0.96      0.98       109\n",
      "               20_newsgroups/sci.space       0.98      0.92      0.95       112\n",
      "  20_newsgroups/soc.religion.christian       0.99      0.97      0.98        77\n",
      "      20_newsgroups/talk.politics.guns       0.94      0.92      0.93        91\n",
      "   20_newsgroups/talk.politics.mideast       0.95      0.85      0.90        97\n",
      "      20_newsgroups/talk.politics.misc       0.93      0.84      0.88       110\n",
      "      20_newsgroups/talk.religion.misc       0.92      0.83      0.87       107\n",
      "\n",
      "                           avg / total       0.94      0.93      0.93      2000\n",
      "\n",
      "[[105   3   0   0   3   0   1   0   1   0   1   0   0   0   1   0   0   1\n",
      "    1   4]\n",
      " [  2 102   0   0   1   0   1   0   0   0   0   0   0   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0  94   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0 101   0   0   1   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0 111   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   2   0   0  85   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0  89   1   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1   1   0   1   4   0   1  84   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1   0   0   1   1   1   1   1 105   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1   2   0   0   4   0   2   0   0 105   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   1   0   1   0  98   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1   0   0   0   0   1   1   0   0   0   0  74   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1   0   0   2   0   0   0   0   0   0   0   0  84   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0   0   2   0   1   0   0   0   0   0   0 105   0   0   0   0\n",
      "    0   0]\n",
      " [  1   4   0   0   0   0   2   0   0   1   0   0   0   1 103   0   0   0\n",
      "    0   0]\n",
      " [  1   0   0   0   0   0   1   0   0   0   0   0   0   0   0  75   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   2   0   3   0   0   0   0   0   0   0   0   0  84   0\n",
      "    1   1]\n",
      " [  2   4   0   1   3   0   2   0   0   0   0   0   0   0   0   1   0  82\n",
      "    2   0]\n",
      " [  1   2   1   0   2   0   3   0   0   0   0   0   0   0   0   0   3   3\n",
      "   92   3]\n",
      " [  4   4   0   0   4   0   0   0   0   0   0   0   1   0   0   0   2   0\n",
      "    3  89]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test,Y_pred))\n",
    "print(confusion_matrix(Y_test,Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
